{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Attention: All the hyperparamters will be kept hidden, but the structure of the model can be observed. If you see {x}, that is a hyperparamter. This is because of all the time and money used to train and test the models."
      ],
      "metadata": {
        "id": "M0kJkuer4v_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGEkm9m11_Tx"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7Eh2NUAIdZz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcnuDh4B2D8p"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHDBAciIgjuA"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM6GpwyUIwr5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from scipy.stats import t\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "import yfinance as yf\n",
        "from datetime import date, datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "from keras.regularizers import l2\n",
        "#from keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBv5IUZyJcl5"
      },
      "outputs": [],
      "source": [
        "Start='2010-09-02'\n",
        "End='2022-08-10'\n",
        "IndexEndDays = yf.download(\"TSLA\",start=Start,  end=End, progress=False).index\n",
        "Lag=3 #how many days in the future you want to predict\n",
        "LagSD=5 #how many past days do you want to include in your standard deviation \n",
        "Dropout={x}\n",
        "LearningRate={x}\n",
        "Epochs={x}; Alpha={x}; Timestep={x}; Batch_Size={x}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFAmlQHBJhLU"
      },
      "outputs": [],
      "source": [
        "#Classifying if the return in the future is positive or negative\n",
        "def classify(future):\n",
        "    if float(future) > 0:  # if the future price is higher than the current, that's a buy, or a 1\n",
        "        return 1\n",
        "    else:  # otherwise... it's a 0!\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzVNofN_Jp9s"
      },
      "outputs": [],
      "source": [
        "#Return calculation from x days before current date where x is Lag\n",
        "def Daily-Return (Database, Lag=1):\n",
        "    dimension=Database.shape[0];Out=np.zeros([dimension-Lag])\n",
        "    for i in range(Lag, dimension):\n",
        "         Out[i - Lag] = (np.log(Database['Close'][i]) - np.log(Database['Close'][i - Lag]))\n",
        "    return np.append(np.repeat(np.nan, Lag),Out), Database.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL2THsqvJrFe"
      },
      "outputs": [],
      "source": [
        "# Standard Deviation Calculation of the past X days where X is the LagSD\n",
        "def STD (DailyReturns, LagSD):\n",
        "    dimension=DailyReturns.shape[0]; dif=LagSD; Out=np.zeros([dimension-dif])\n",
        "    for i in range(dif, dimension):\n",
        "        Out[i - dif]=np.std(DailyReturns[i-dif:i],ddof=1)\n",
        "    return np.append(np.repeat(np.nan, dif),Out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLQKQKU4Jxb1"
      },
      "outputs": [],
      "source": [
        "#Calculates the closing return X days in the future where X is Lag\n",
        "def Future-Return (Database, Lag):\n",
        "    dimension=Database.shape[0];Out=np.zeros([dimension-Lag])\n",
        "    for i in range(dimension-Lag):\n",
        "         Out[i] = (np.log(Database['Close'][i + Lag]) - np.log(Database['Close'][i]))\n",
        "    return np.append(Out,np.repeat(np.nan, Lag)), Database.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_moCVyzmgpe"
      },
      "outputs": [],
      "source": [
        "#Generates Database for 438 trading days of TSLA, SPY, APPL, and VIX closing returns and volume, and also standard deviation\n",
        "def DataCreation (Lag, IndexEndDays, LagSD, i):\n",
        "    DatabaseT = yf.download(\"TSLA\",start= IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(), progress=False)\n",
        "    DatabaseT.dropna(inplace=True)\n",
        "    DatabaseS = yf.download(\"SPY\", start=IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(),\n",
        "                           progress=False)\n",
        "    DatabaseS.dropna(inplace=True)\n",
        "    DatabaseA = yf.download(\"AAPL\", start=IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(),\n",
        "                           progress=False)\n",
        "    DatabaseA.dropna(inplace=True)\n",
        "    DatabaseV = yf.download(\"^VIX\", start=IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(),\n",
        "                           progress=False)\n",
        "    DatabaseV.dropna(inplace=True)\n",
        "    DailyReturnsOld, Index = Future-Return(DatabaseT, Lag)\n",
        "    DailyReturnsT, Index = Daily-Return(DatabaseT)\n",
        "    DailyReturnsS, Index = Daily-Return(DatabaseS)\n",
        "    DailyReturnsA, Index = Daily-Return(DatabaseA)\n",
        "    DailyReturnsV, Index = Daily-Return(DatabaseV)\n",
        "    SD = STD(DailyReturnsT, LagSD)\n",
        "    Data = pd.DataFrame({'TSLA_Day': DailyReturnsT, 'TSLA_Volume': DatabaseT['Volume'], 'APPL_Day': DailyReturnsA, 'APPL_Volume': DatabaseA['Volume'], 'VIX_Day': DailyReturnsV,  'SD': SD, 'SPY_Day': DailyReturnsS, 'SPY_Volume': DatabaseS['Volume'], 'DailyReturnsOld': DailyReturnsOld})\n",
        "    Data = Data.set_index(Index)\n",
        "    Data.dropna(inplace=True)\n",
        "    Data['Target'] = list(map(classify, Data['DailyReturnsOld']))\n",
        "    Data = Data.drop(\"DailyReturnsOld\", 1)\n",
        "    return Data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mp5z9D-yvJz"
      },
      "outputs": [],
      "source": [
        "#Generating the same Database without Future-Return or Target\n",
        "def Forecast(Lag, IndexEndDays, LagSD, i):\n",
        "    DatabaseT = yf.download(\"TSLA\", start= IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(), progress=False)\n",
        "    DatabaseT.dropna(inplace=True)\n",
        "    DatabaseS = yf.download(\"SPY\", start= IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(), progress=False)\n",
        "    DatabaseS.dropna(inplace=True)\n",
        "    DatabaseA = yf.download(\"AAPL\", start= IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(), progress=False)\n",
        "    DatabaseA.dropna(inplace=True)\n",
        "    DatabaseV = yf.download(\"^VIX\", start= IndexEndDays[i].date(), end=IndexEndDays[i + 438].date(), progress=False)\n",
        "    DatabaseV.dropna(inplace=True)\n",
        "    DailyReturnsT, Index = Daily-Return(DatabaseT)\n",
        "    DailyReturnsS, Index = Daily-Return(DatabaseS)\n",
        "    DailyReturnsA, Index = Daily-Return(DatabaseA)\n",
        "    DailyReturnsV, Index = Daily-Return(DatabaseV)\n",
        "    SD = STD(DailyReturnsT, LagSD)\n",
        "    Data = pd.DataFrame({'TSLA_Day': DailyReturnsT, 'TSLA_Volume': DatabaseT['Volume'], 'APPL_Day': DailyReturnsA, 'APPL_Volume': DatabaseA['Volume'], 'VIX_Day': DailyReturnsV,  'SD': SD, 'SPY_Day': DailyReturnsS, 'SPY_Volume': DatabaseS['Volume']})\n",
        "    Data = Data.set_index(Index)\n",
        "    return Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AJX4eQvmkOk"
      },
      "outputs": [],
      "source": [
        "#It generates the database for fitting transformer. No positional encoding is needed as LSTM plays this role in the model structure\n",
        "#It basically formulates sequences to input into the lstm and changes your data from [number_of_samples, number_of_features] to [number_of_samples, seq_length, number_of_features]\n",
        "def ForecastLSTM_Formatter (Timestep, XData_AR, YData_AR):\n",
        "    Features = XData_AR.shape[1]; Sample = XData_AR.shape[0]-Timestep+1\n",
        "    XDataTrainScaledRNN=np.zeros([Sample, Timestep, Features]); YDataTrainRNN=np.zeros([Sample])\n",
        "    for i in range(Sample):\n",
        "        XDataTrainScaledRNN[i,:,:] = XData_AR[i:(Timestep+i)]\n",
        "        YDataTrainRNN[i] = YData_AR[Timestep+i-1]\n",
        "    return XDataTrainScaledRNN, YDataTrainRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Following Classes, MultiHeadSelfAttention and TransformerBlock are taken from https://arxiv.org/pdf/2109.12621.pdf"
      ],
      "metadata": {
        "id": "1UiS2t1KKkqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEYR9eFEoZVo"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
        "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(value, batch_size)  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(concat_attention)  # (batch_size, seq_len, embed_dim)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hexBMDZNoVbq"
      },
      "outputs": [],
      "source": [
        "# Transformer Keras Block\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.nb_dict = {};\n",
        "        self.Bagging = {x}\n",
        "        #check this bagging too\n",
        "        for i in range(self.Bagging):\n",
        "            self.nb_dict[\"att{0}\".format(i)] = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim), ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        self.att_dict = {}\n",
        "        for i in range(self.Bagging):\n",
        "            self.att_dict[\"att{0}\".format(i)] = self.nb_dict[\"att{0}\".format(i)](tf.keras.layers.Dropout(.1)(inputs))\n",
        "            if i == 0:\n",
        "                self.att_dict[\"attn_output\"] = self.att_dict[\"att{0}\".format(i)] / self.Bagging\n",
        "            else:\n",
        "                self.att_dict[\"attn_output\"] = self.att_dict[\"attn_output\"] + self.att_dict[\n",
        "                    \"att{0}\".format(i)] / self.Bagging\n",
        "        attn_output = self.dropout1(self.att_dict[\"attn_output\"], training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmQcwLF-J0ih"
      },
      "outputs": [],
      "source": [
        "def Transformer_Model(Shape1, Shape2, HeadsAttention, Dropout, LearningRate):\n",
        "    # Model struture is defined\n",
        "    Input = tf.keras.Input(shape=(Shape1, Shape2), name=\"Input\")\n",
        "    # LSTM is applied on top of the transformer\n",
        "    X = tf.keras.layers.LSTM(units={x}, activation='tanh', dropout=Dropout, recurrent_dropout={x}, kernel_regularizer=l2({x}), recurrent_regularizer=l2({x}), bias_regularizer=l2({x}), return_sequences=True)(Input)\n",
        "    X = tf.keras.layers.LSTM(units={x}, activation='tanh', dropout=Dropout, recurrent_dropout={x}, kernel_regularizer=l2({x}), recurrent_regularizer=l2({x}), bias_regularizer=l2({x}), return_sequences=True)(Input)\n",
        "    # Tranformer architecture is implemented\n",
        "    transformer_block_1 = TransformerBlock(embed_dim=32, num_heads=HeadsAttention, ff_dim=8, rate=Dropout)\n",
        "    X = transformer_block_1(X)\n",
        "    # Dense layers are used\n",
        "    # X = BatchNormalization()(X)\n",
        "    X = tf.keras.layers.GlobalAveragePooling1D()(X)\n",
        "    X = tf.keras.layers.Dense({x}, activation=tf.nn.sigmoid)(X)\n",
        "    X = tf.keras.layers.Dropout(Dropout)(X)\n",
        "    Output = tf.keras.layers.Dense({x}, activation='sigmoid', name=\"Output\")(X)\n",
        "    #Dense layers are used\n",
        "    model = tf.keras.Model(inputs=Input, outputs=Output)\n",
        "    # Optimizer is defined\n",
        "    Opt = tf.keras.optimizers.Nadam(learning_rate=LearningRate, beta_1={x}, beta_2={x}, epsilon={x}, name='Nadam')\n",
        "    # Model is compiled\n",
        "    model.compile(optimizer=Opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA-O4vO_m6e_"
      },
      "outputs": [],
      "source": [
        "#Fitting of Transformed LSTM Model of Up or Down in 5 days\n",
        "model = Transformer_Model(Timestep, 8, HeadsAttention=8, Dropout=0.1, LearningRate=LearningRate)\n",
        "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast': [], 'Loss': []})\n",
        "#model = keras.models.load_model('example_model_path')\n",
        "#ResultsCollection = pd.read_csv ('example_csv_path')\n",
        "\n",
        "for i in tqdm(range(2566)):\n",
        "    #Database is downloaded from yahoo finance and lag of returns defined\n",
        "    Data = DataCreation (Lag, IndexEndDays, LagSD, i)\n",
        "    XData_AR = Data.drop(Data.columns[[8]], axis=1)\n",
        "    YData_AR = Data['Target']\n",
        "    Scaled_Norm = preprocessing.StandardScaler().fit(XData_AR) #StandardScaler() will normalize the features i.e. each column of X, INDIVIDUALLY, so that each column or feature will have μ = 0 and σ = 1\n",
        "    XData_AR_Norm = Scaled_Norm.transform(XData_AR)\n",
        "    XData_AR_Norm_T, YData_AR_Norm_T = ForecastLSTM_Formatter(Timestep, XData_AR_Norm, YData_AR)\n",
        "    #Model predicting on Past Test data if the return in 3 days positive or negative \n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience={x}) #epoch dynamically changes and each training iteration 'stops early' when the loss from epoch to epoch increases instead of decreasing x times\n",
        "    history = model.fit(XData_AR_Norm_T, YData_AR_Norm_T, epochs=Epochs, batch_size=Batch_Size, callbacks=[callback], verbose=0)\n",
        "    model.save('example_model_path') \n",
        "    #Adding the next trading day without doing Future-Return on it so the model doesn't know if its 1 or 0\n",
        "    XData_Forecast = Forecast(Lag, IndexEndDays, LagSD, i)\n",
        "    Index_Forecast = XData_Forecast.index[(-Lag)]\n",
        "    XData_Forecast = XData_Forecast.iloc[[(-Lag)]]\n",
        "    XDataForecast = pd.concat([XData_AR,XData_Forecast])\n",
        "    XDataForecast = XDataForecast.iloc[1: , :]\n",
        "    XDataForecastTotalScaled = Scaled_Norm.transform(XDataForecast)\n",
        "    XDataForecastTotalScaled_T, Y_T = ForecastLSTM_Formatter(Timestep, XDataForecastTotalScaled, np.zeros(XDataForecastTotalScaled.shape[0]))\n",
        "    #Model predicting if the next trading day will have a positive or negative return in 3 days\n",
        "    TransformerPrediction = model.predict(XDataForecastTotalScaled_T, batch_size=Batch_Size)\n",
        "    IterResults={'Date_Forecast': Index_Forecast, 'Forecast' : TransformerPrediction[-1], 'Loss': history.history['loss'] }\n",
        "    print(history.history['loss'])\n",
        "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
        "    #Results are saved\n",
        "    ResultsCollection.to_csv('example_csv_path',index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Non-Stateful-Model-3dayLag.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}